# 目录

<!--自动插入TOC：https://github.com/ekalinin/github-markdown-toc-->
<!--ts-->
   * [目录](#目录)
   * [基本概念](#基本概念)
      * [灵敏度/特异度](#灵敏度特异度)
   * [模型](#模型)
      * [数据预处理(归一化)](#数据预处理归一化)
      * [不均衡数据集处理](#不均衡数据集处理)
      * [SVM](#svm)
   * [学习资料](#学习资料)
      * [视频资料](#视频资料)

<!-- Added by: luyl, at: 2018-12-03T17:14+08:00 -->

<!--te-->

----

# 基本概念

## 灵敏度/特异度

[Sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

* 真阳性率(true positive rate, TPR)，又称为灵敏度(sensitivity)，即有病诊断阳性的概率   TPR = TP/(TP+FN)
* 真阴性率(true negative rate)，又称为特异度(specificity, SPC), 即有病诊断阳性的概率   SPC = TN/(TN+FP)
* 假阳性率(false positive rate, FPR)，又称为"误诊率"   FPR = FP/(FP+TN) = 1 - SPC
* 假阴性率(false negative rate, FNR)，又称为"漏诊率"   FNR = FN/(TP+FN) = 1 - TPR
* 阳性预测值(positive predictive value, PPV), 即诊断为阳性中有病的概率    PPV = TP/(TP+FP)
* 隐性预测值(negative predictive value, NPV), 即诊断为阴性中无病的概率    NPV = TN/(TN+FN)

灵敏度：实际实际无病的人正确判断为真阳性的比例

特异度：实际有病的人正确判断为真阴性的比例

----

# 模型

## 数据预处理(归一化)

* [处理离散型特征和连续型特征共存的情况 归一化 论述了对离散特征进行one-hot编码的意义](https://blog.csdn.net/lujiandong1/article/details/49448051)
* [利用python对包含离散型特征和连续型特征的数据进行预处理](https://blog.csdn.net/wotui1842/article/details/80697444)

拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，
特征B的取值范围是[-1,1].如果使用logistic回归，w1*x1+w2*x2，因为x1的取值太大了，所以x2
基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。

**连续型特征归一化的常用方法**

1. 线性放缩到[-1,1]
2. 线性放缩到[-1,1]

**离散型特征的处理方法**

离散型特征就是该特征对应的值不能进行大小比较，比如：职业，性别，国家等等

对于离散的特征基本就是按照one-hot编码，该离散特征有多少取值，就用多少维来表示该特征。
为什么使用one-hot编码来处理离散型特征，这是有理由的，不是随便拍脑袋想出来的！！！具体原因，
分下面几点来阐述： 

1. 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。
2. 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，
特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式
空间的相似度计算，计算余弦相似性，基于的就是欧式空间。
3. 离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。比如，有一个离散型特征，
代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), 
x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。
那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用
one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距
离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理
4. 将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用
one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码，比如，该离散特征共有1000
个取值，我们分成两组，分别是400和600,两个小组之间的距离有合适的定义，组内的距离也有合适的定
义，那就没必要用one-hot 编码
 
离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对
连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1

基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。基于参数的模型或基
于距离的模型，都是要进行特征的归一化。


## 不均衡数据集处理

* [不平衡数据集下的SVM算法研究](https://blog.csdn.net/u011414200/article/details/47310795)

从技术角度上说，任何在不同类之间展现出不等分布的样本集都应该被认为是不均衡的，并且应该展现出
明显的不平衡特征。具体来说，这种不均衡形式被称为类间不均衡，常见的多数类与少数类比例是100:1，
1000:1，10000:1

有时对少数类错分情况的后果很严重，比如癌症患者被误诊为健康人。所以需要的分类器应该是在不严重
损失多数类精度的情况下，在少数类上获得尽可能高的精度

同时也暗示着使用单一的评价准则，例如全局精度或是误差率，是不能给不均衡问题提供足够的评价信息。
因此利用含有更多信息的评价指标，例如接收机特性曲线、精度-recall曲线和代价曲线

样本不均衡的程度不是阻碍分类学习的唯一因素，样本集的复杂度也是导致分类性能恶化的重要因素，
另外相对不平衡比例的增大也可能使分类性能进一步恶化 。其中样本集复杂度是广义的术语，它包括重叠、
缺少代表性样本、类别间分离程度小等



## SVM

* [SVM支持向量机分类模型SVC理论+python sklean.svm实践](https://blog.csdn.net/SummerStoneS/article/details/78551757)
* [SVM详解(包含它的参数C为什么影响着分类器行为)-scikit-learn拟合线性和非线性的SVM](https://blog.csdn.net/xlinsist/article/details/51311755)


**核映射与核函数**

通过核函数，支持向量机可以将特征向量映射到更高维的空间中，使得原本线性不可分的数据在映射之后的
空间中变得线性可分。假设原始向量为x，映射之后的向量为z，这个映射为：

> z = φ(x)

在实现时不需要直接对特征向量做这个映射，而是用核函数对两个特征向量的内积进行变换，这样做等价于
先对向量进行映射然后再做内积：

> K(x<sub>i</sub>, x<sub>j</sub>) = K(x<sub>i</sub><sup>T</sup>x<sub>j</sub>) = φ(x<sub>i</sub>)<sup>T</sup>φ(x<sub>j</sub>)

在这里K为核函数。常用的非线性核函数有多项式核，高斯核（也叫径向基函数核，RBF）。下表列出了各种
核函数的计算公式：

| 核函数 | 计算公式 |
| ---- | ---- |
| 线性核 | K(x<sub>i</sub>, x<sub>j</sub>) = x<sub>i</sub><sup>T</sup>x<sub>j</sub> |
| 多项式核 | K(x<sub>i</sub>, x<sub>j</sub>) = (γx<sub>i</sub><sup>T</sup>x<sub>j</sub> + b)<sup>d</sup> |
| 径向基函数(高斯)核 | K(x<sub>i</sub>, x<sub>j</sub>) = exp(-γ\|\|x<sub>i</sub> - x<sub>j</sub>\|\|<sup>2</sup>) |
| sigmoid核 | K(x<sub>i</sub>, x<sub>j</sub>) = tanh(γx<sub>i</sub><sup>T</sup>x<sub>j</sub> + b) |


其中，γ(核系数), b(独立项)，d(深度)为人工设置的参数，d是一个正整数，为正实数，b为非负实数。

尽管最佳核函数的选择一般与问题自身有关，但对普遍问题还是有规律可循的，建议初学者在通常情况下，
优先考虑径向基核函数（RBF）.主要基于以下考虑：

1. 作为一种对应于非线性映射的核函数，RBF 能够处理非线性可分的问题

2. 线性核函数时 RBF 核函数的一种特例，即通过适当地选择参数 (γ,C)(γ,C)，RBF 核函数总可以得到与错误代价参数 
C 的线性核函数相同的效果，反之当然不成立

3. 在选择某些参数的情况下，Sigmoid 核函数的行为也类似于 RBF 核函数,而且选择 Sigmoid 核函数就有 2 个与之有
关的参数 b、c 需要确定。

4. 多项式核函数需要计算内积，而这有可能产生溢出之类的计算问题。



----

# 学习资料

## 视频资料

* 斯坦福大学机器学习-吴恩达, [学习笔记](https://blog.csdn.net/hujingshuang/article/category/3277895)
